{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation for Excel Sheets to be applied to the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import string \n",
    "import pandas_datareader.data as web\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def dataset(df,number,ticker):\n",
    "    df['Date'] = df['Date'].astype('datetime64[ns]') #converting date to datetime format to properly combine dataframes\n",
    "    df = df.set_index('Date') #setting the index to concat on date\n",
    "    df = df.drop(['Label'],axis=1)\n",
    "    new = [] #creating an empty list to be used during the for loop below\n",
    "    for column in df.columns: #loop through the 25 columns to prepare for formatting the strings\n",
    "        df[column] = (df[column].str.replace(\"b'\", \"\")) #cleaning the strings of this random letter b\"\n",
    "        new.append(df[column].str.replace('b\"', \"\")) #appending the cleaned strings to a list\n",
    "    main = pd.DataFrame(data = new).T #transposing the dataset for proper formating \n",
    "    stock = web.DataReader(ticker,'yahoo','2008-08-08','2016-07-01')[['Adj Close','High','Low']] #getting adj close price data of SPY\n",
    "    data = pd.concat([main,stock],axis=1) #concatenating the two dataframes\n",
    "    data['Target'] = data['Adj Close'].pct_change()*100 #getting the return of the stock \n",
    "    data.loc[data['Target']<(0),'Label'] = 0 #negative returns are 0\n",
    "    data.loc[data['Target']>(0),'Label'] = 1 #positive returns are 1\n",
    "    \n",
    "    #was  used for the multi-classification model \n",
    "    #data.loc[data['Target']<(-3.0),'Label'] = 0 #worst returns are 0\n",
    "    #data.loc[(data['Target']>=(-3.0)) & (data['Target']<(-1.5)),'Label'] = 1 #terrible returns are 1 \n",
    "    #data.loc[(data['Target']>=(-1.5)) & (data['Target']<0),'Label'] = 2 #bad returns are 2\n",
    "    #data.loc[(data['Target']>=(0.0)) & (data['Target']<1.5),'Label'] = 3 #decent returns are 3\n",
    "    #data.loc[(data['Target']>=(1.5)) & (data['Target']<=3.0),'Label'] = 4 #good returns are 4 \n",
    "    #data.loc[data['Target']>3.0,'Label'] = 5 #highest returns are 5\n",
    "    \n",
    "    data = data.drop(['Adj Close','High','Low','Target'],axis =1) #drop these bc we're solving for return labels\n",
    "    headlines = data.columns[:number] #number of top headlines for the day\n",
    "    data['combined'] = data[headlines].apply(lambda row: '. '.join(row.values.astype(str)), axis=1) #combining the headlines\n",
    "    data = data.drop(headlines,axis = 1) #dropping the columns that I merged\n",
    "    return data.dropna() #no null values for TensorFlow\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation) #removing all punctuation in a string \n",
    "    return text.translate(translator) \n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in set(stopwords.words(\"english\"))] #converting all letters to lowercase and removing if they are classified as stop words\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "#count the number of words in a string using collections Counter\n",
    "def counter(string):\n",
    "    number = Counter() #calling Counter from collections \n",
    "    for i in string.values: #looping through each string \n",
    "        for word in i.split(): #looping through each word in the string \n",
    "            number[word] += 1 #counting everything there is a word\n",
    "    return number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary NLP Model [25 headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "44/44 - 9s - loss: 0.6902 - accuracy: 0.5505 - val_loss: 0.6866 - val_accuracy: 0.5613\n",
      "Epoch 2/3\n",
      "44/44 - 2s - loss: 0.6457 - accuracy: 0.5938 - val_loss: 0.7080 - val_accuracy: 0.5042\n",
      "Epoch 3/3\n",
      "44/44 - 2s - loss: 0.3328 - accuracy: 0.9185 - val_loss: 0.9558 - val_accuracy: 0.5277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x145e5e890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_model(df,number,max_length):\n",
    "    y,x = df['Label'],df['combined'] #label and feature\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=50,test_size=0.3) #training and testing\n",
    "    tokenizer = Tokenizer(num_words=number) #calling the tokenizer to fit the unique amount of words in the dataset\n",
    "    tokenizer.fit_on_texts(x_train) # fit integers to training headlines\n",
    "    x_train_seq = tokenizer.texts_to_sequences(x_train) #converting x_train to sequences given the total number of unique words\n",
    "    x_test_seq = tokenizer.texts_to_sequences(x_test) #converting x_test to sequences\n",
    "    padded_x_train = pad_sequences(x_train_seq, maxlen=max_length, padding=\"post\", truncating=\"post\") #padding the training features \n",
    "    padded_x_test = pad_sequences(x_test_seq, maxlen=max_length, padding=\"post\", truncating=\"post\") #padding the testing features\n",
    "    model = keras.models.Sequential() #sequential model\n",
    "    model.add(layers.Embedding(number, 32, input_length=max_length)) #embedding layer on the total number of unique words\n",
    "    model.add(layers.LSTM(64, dropout=0.1)) #lstm layer with a 10% dropout\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\")) # binary classification problem\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer='Adam', metrics=[\"accuracy\"])\n",
    "    model.fit(padded_x_train, y_train, epochs=3, validation_data=(padded_x_test, y_test), verbose=2)\n",
    "    return model\n",
    "\n",
    "#all 25 headlines\n",
    "df = pd.read_excel('nlp_excel_binary.xlsx') #getting the dataset that was saved from the dataset function\n",
    "number = counter(df.combined) #to properly tokenize\n",
    "number = len(number) #length of the number of unique words for the tokenizer\n",
    "binary_model(df,number,32) #binary model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary NLP Model [5 headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryanfinegan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "44/44 - 7s - loss: 0.6920 - accuracy: 0.5303 - val_loss: 0.6861 - val_accuracy: 0.5613\n",
      "Epoch 2/3\n",
      "44/44 - 3s - loss: 0.6552 - accuracy: 0.5866 - val_loss: 0.6890 - val_accuracy: 0.5277\n",
      "Epoch 3/3\n",
      "44/44 - 3s - loss: 0.4117 - accuracy: 0.8939 - val_loss: 0.7511 - val_accuracy: 0.4874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x147495e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import string \n",
    "import pandas_datareader.data as web\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('Combined_News_DJIA.csv') #was a csv with top 25 world news headlines\n",
    "ticker = 'spy'\n",
    "number = 5\n",
    "def data(df,ticker,number):\n",
    "    df['Date'] = df['Date'].astype('datetime64[ns]') #converting date to datetime format to properly combine dataframes\n",
    "    df = df.set_index('Date') #setting the index to concat on date\n",
    "    df = df.drop(['Label'],axis =1)\n",
    "    new = [] #creating an empty list to be used during the for loop below\n",
    "    for column in df.columns: #loop through the 25 columns to prepare for formatting the strings\n",
    "        df[column] = (df[column].str.replace(\"b'\", \"\")) #cleaning the strings of this random letter b\"\n",
    "        new.append(df[column].str.replace('b\"', \"\")) #appending the cleaned strings to a list\n",
    "    main = pd.DataFrame(data = new).T #transposing the dataset for proper formating \n",
    "    stock = web.DataReader(ticker,'yahoo','2008-08-08','2016-07-01')[['Adj Close','High','Low']] #getting adj close price data of SPY\n",
    "    data = pd.concat([main,stock],axis=1) #concatenating the two dataframes\n",
    "    data['Target'] = data['Adj Close'].pct_change()*100 #getting the return of the stock \n",
    "    data.loc[data['Target']<(0),'Label'] = 0 #negative returns are 0\n",
    "    data.loc[data['Target']>(0),'Label'] = 1 #positive returns are 1\n",
    "    data = data.drop(['Adj Close','High','Low','Target'],axis =1) #drop these bc we're solving for return labels\n",
    "    headlines = data.columns[:-1] #number of top headlines for the day\n",
    "    data['combined'] = data[headlines].apply(lambda row: '. '.join(row.values.astype(str)), axis=1) #combining the headlines\n",
    "    data = data.drop(headlines,axis = 1) #dropping the columns that I merged\n",
    "    return data.dropna()\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation) #removing all punctuation in a string \n",
    "    return text.translate(translator) \n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in set(stopwords.words(\"english\"))] #converting all letters to lowercase and removing if they are classified as stop words\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "#count the number of words in a string using collections Counter [Tensorflow tutorial]\n",
    "def counter(string):\n",
    "    number = Counter() #calling Counter from collections \n",
    "    for i in string.values: #looping through each string \n",
    "        for word in i.split(): #looping through each word in the string \n",
    "            number[word] += 1 #counting everything there is a word\n",
    "    return number\n",
    "\n",
    "df = data(pd.read_csv('Combined_News_DJIA.csv'),ticker,number)\n",
    "number = counter(df.combined) #to properly tokenize\n",
    "number = len(number) #length of the number of unique words for the tokenizer\n",
    "binary_model(pd.read_excel('nlp_5headline.xlsx'),number,32) #binary model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass NLP Model [25 Headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "44/44 - 6s - loss: 1.4395 - accuracy: 0.4486 - val_loss: 1.2248 - val_accuracy: 0.4807\n",
      "Epoch 2/3\n",
      "44/44 - 2s - loss: 1.1919 - accuracy: 0.4687 - val_loss: 1.2049 - val_accuracy: 0.4807\n",
      "Epoch 3/3\n",
      "44/44 - 2s - loss: 1.0005 - accuracy: 0.5191 - val_loss: 1.3243 - val_accuracy: 0.4255\n"
     ]
    }
   ],
   "source": [
    "def multiclass_model(df,number,max_length):\n",
    "    y,x = df['Label'],df['combined'] #label and feature\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=50,test_size=0.3) #training and testing (shuffling)\n",
    "    tokenizer = Tokenizer(num_words=number) #calling the tokenizer to fit the unique amount of words in the dataset\n",
    "    tokenizer.fit_on_texts(x_train) # fit integers to training headlines\n",
    "    x_train_seq = tokenizer.texts_to_sequences(x_train) #x_train to sequences\n",
    "    x_test_seq = tokenizer.texts_to_sequences(x_test) #x_test to sequences\n",
    "    padded_x_train = pad_sequences(x_train_seq, maxlen=max_length, padding=\"post\", truncating=\"post\") #padding the sequences\n",
    "    padded_x_test = pad_sequences(x_test_seq, maxlen=max_length, padding=\"post\", truncating=\"post\") \n",
    "    model = keras.models.Sequential() #sequential model \n",
    "    model.add(layers.Embedding(number, 32, input_length=max_length)) #embedding layer provides weights to integer (words)\n",
    "    model.add(layers.LSTM(64, dropout=0.2)) #LSTM layer with a 20% dropout\n",
    "    model.add(layers.Dense(6, activation=\"softmax\")) #softmax for probability of the six classes (6 in the output layer for classes)\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer='Adam', metrics=[\"accuracy\"])\n",
    "    model.fit(padded_x_train, y_train, epochs=3, validation_data=(padded_x_test, y_test), verbose=2)\n",
    "    \n",
    "df = pd.read_excel('nlp_excel_data.xlsx') #dataframe created and saved with the dataset function\n",
    "number = counter(df.combined) #to properly tokenize\n",
    "number = len(number) #length of the number of unique words for the tokenizer\n",
    "multiclass_model(df,number,32) #multiclassification model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
